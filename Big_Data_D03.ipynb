{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yoXae8L2fAP"
      },
      "source": [
        "## BD10 - D03 - sander: A Predictive Classification Model for Affective Texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5DMmxbV5b2i"
      },
      "source": [
        "### Introduction\n",
        "- **It may take hours to run the code**\n",
        "- **Group name**: Pesticide\n",
        "- **Group members**: \n",
        "  - Can Zhou (20031818)\n",
        "  - Xuting Wu (20028395)\n",
        "  - Jizhou Che (20032291)\n",
        "  - Yingrui Ma (20032288)\n",
        "\n",
        "- **Dataset: sander**\n",
        "- **How to run**:\n",
        "  - **Download and put the folder in your root path of Google Drive!!!**\n",
        "  - Root Path: /content/drive/My Drive/\n",
        "  - Floder information:\n",
        "    - Big_Data_CW_BD10\n",
        "      - README.txt\n",
        "      - Big_Data_D01.ipynb\n",
        "      - Big_Data_D02.ipynb\n",
        "      - Big_Data_D03.ipynb\n",
        "      - Dataset_Verification_Label_Accuracy.ipynb\n",
        "      - Dataset_Verification_Spelling_Check.ipynb\n",
        "      - Chart.ipynb\n",
        "      - Datasets\n",
        "        - STS.csv\n",
        "        - tweet_emotions.csv\n",
        "        - sander.csv\n",
        "      - HelperFile\n",
        "        - abbr.txt\n",
        "    - Please contact us if there is any confusion.\n",
        "\n",
        "\n",
        "- **Code structure**:\n",
        "  - Install and Import\n",
        "  - Start & Read Data & Select Classes\n",
        "  - Stage01: Text Preprocessing\n",
        "    - Step01: Spelling Correction; Abbreviation & Emoji Recovery\n",
        "    - Step02: Tokenisation\n",
        "    - Step03: Stopwords & Noise Removal\n",
        "    - Step04: Word Lemmatisation & Normalisation\n",
        "    - Pipeline Construction\n",
        "  - Stage02: Feature Extraction\n",
        "    - TF-IDF\n",
        "    - word2vec\n",
        "    - Bert\n",
        "  - Divide Data\n",
        "  - Stage03: Model Training\n",
        "    - Logistic Regression\n",
        "    - Linear SVM\n",
        "    - Naive Bayes\n",
        "    - MLP\n",
        "  - Experiments\n",
        "    - Experiments Setup\n",
        "    - Proposed Framework Experiments\n",
        "    - Baseline Framework Experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9V9q6G422Y6"
      },
      "source": [
        "### Install and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw7B8tdB2KUs",
        "outputId": "62e3d367-0d2b-4c18-c2aa-60cc02742b57"
      },
      "source": [
        "# Install java\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "!java -version\n",
        "\n",
        "# Install pyspark\n",
        "!pip install --ignore-installed -q pyspark\n",
        "\n",
        "# Install Sparknlp\n",
        "!pip install --ignore-installed spark-nlp\n",
        "\n",
        "# Install helpers\n",
        "!apt-get install enchant\n",
        "!pip install pyenchant"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"11.0.10\" 2021-01-19\n",
            "OpenJDK Runtime Environment (build 11.0.10+9-Ubuntu-0ubuntu1.18.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.10+9-Ubuntu-0ubuntu1.18.04, mixed mode, sharing)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 72kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 48.6MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spark-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/35/3d06b93fefdeab0f6f544b1fc48e5e49c049697c38611ef870383031380b/spark_nlp-3.0.3-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-3.0.3\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 79 not upgraded.\n",
            "Need to get 1,310 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.1 [309 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.1 [87.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Fetched 1,310 kB in 1s (1,000 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 161033 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Collecting pyenchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8c/bd224a5db562ac008edbfaf015f5d5c98ea13e745247cd4ab5fc5b683085/pyenchant-3.2.0-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ju7RlYs3Mi2"
      },
      "source": [
        "# Import\n",
        "import enchant\n",
        "from enchant.checker import SpellChecker\n",
        "from enchant.tokenize import EmailFilter, URLFilter\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "from pyspark.sql.functions import col, lit, udf, split, explode\n",
        "from pyspark.sql.types import StringType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml.feature import SQLTransformer, StopWordsRemover, Word2Vec, HashingTF, IDF, Tokenizer, MinMaxScaler\n",
        "from pyspark.ml.classification import LinearSVC, LogisticRegression, NaiveBayes, MultilayerPerceptronClassifier\n",
        "\n",
        "import sparknlp\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.common import *\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from sparknlp import Finisher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2whyn-RGkjgw"
      },
      "source": [
        "**Import Colab, please pay attention to the path!!!!**  \n",
        "**If the code is run in cslinux, please do not import colab and change path to:**  \n",
        "```\n",
        "path01 = \"Datasets/\"  \n",
        "path02 = \"HelperFile/\"\n",
        "```\n",
        "**under the Big_Data_CW_BD10 folder path**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRyY-ewO4wV5",
        "outputId": "ba32a814-bd43-41a4-d11e-b98aff368e6e"
      },
      "source": [
        "# Only for colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path01 = \"/content/drive/My Drive/Big_Data_CW_BD10/Datasets/\"\n",
        "path02 = \"/content/drive/My Drive/Big_Data_CW_BD10/HelperFile/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwxohxwb6IWu"
      },
      "source": [
        "### Start & Read data & Select Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csMpOwUl6L3w"
      },
      "source": [
        "# Start\n",
        "spark = sparknlp.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Znc0Gwb7Azc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f847769-4c74-4a6c-858a-92a2d29b0101"
      },
      "source": [
        "# Read data\n",
        "data = spark.read.csv(path01 + 'sander.csv', header=True)\n",
        "data = data.withColumnRenamed(\"TweetText\", \"original_text\")\n",
        "data.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---------+------------------+--------------------+----------------------+\n",
            "|Topic|Sentiment|           TweetId|           TweetDate|         original_text|\n",
            "+-----+---------+------------------+--------------------+----------------------+\n",
            "|apple| positive|126415614616154112|Tue Oct 18 21:53:...|  Now all @Apple ha...|\n",
            "|apple| positive|126404574230740992|Tue Oct 18 21:09:...|  @Apple will be ad...|\n",
            "|apple| positive|126402758403305474|Tue Oct 18 21:02:...|  Hilarious @youtub...|\n",
            "|apple| positive|126397179614068736|Tue Oct 18 20:40:...|  @RIM you made it ...|\n",
            "|apple| positive|126395626979196928|Tue Oct 18 20:34:...|  I just realized t...|\n",
            "|apple| positive|126394830791254016|Tue Oct 18 20:30:...|  I'm a current @Bl...|\n",
            "|apple| positive|126379685453119488|Tue Oct 18 19:30:...|  The 16 strangest ...|\n",
            "|apple| positive|126377656416612353|Tue Oct 18 19:22:...|  Great up close & ...|\n",
            "|apple| positive|126373779483004928|Tue Oct 18 19:07:...|  From which compan...|\n",
            "|apple| positive|126366353757179904|Tue Oct 18 18:37:...|  Just apply for a ...|\n",
            "|apple| positive|126366123368267776|Tue Oct 18 18:36:...|  RT @JamaicanIdler...|\n",
            "|apple| positive|126365858481188864|Tue Oct 18 18:35:...|  Lmao I think @app...|\n",
            "|apple| positive|126360935509135362|Tue Oct 18 18:16:...|  RT @PhillipRowntr...|\n",
            "|apple| positive|126360398885687296|Tue Oct 18 18:14:...|  Wow. Great deals ...|\n",
            "|apple| positive|126358340220616704|Tue Oct 18 18:05:...|  Just registered a...|\n",
            "|apple| positive|126357982685569024|Tue Oct 18 18:04:...|你好 ! Currently le...|\n",
            "|apple| positive|126352268705538048|Tue Oct 18 17:41:...|  Come to the dark ...|\n",
            "|apple| positive|126350302113824769|Tue Oct 18 17:33:...|  Hey @apple, if yo...|\n",
            "|apple| positive|126349695676203009|Tue Oct 18 17:31:...|  Thank you @apple ...|\n",
            "|apple| positive|126342268603998208|Tue Oct 18 17:01:...|  Thanks to @Apple ...|\n",
            "+-----+---------+------------------+--------------------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbPPbfmNJhKl"
      },
      "source": [
        "### Select Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrvQ7AUDKMEB",
        "outputId": "d143bf87-1a15-40c3-94f0-8d313a54e39d"
      },
      "source": [
        "positive_df = data.filter(data.Sentiment == \"positive\").select(\"original_text\")\n",
        "negative_df = data.filter(data.Sentiment == \"negative\").select(\"original_text\")\n",
        "positive_df = positive_df.limit(250)\n",
        "negative_df = negative_df.limit(250)\n",
        "\n",
        "data = positive_df.withColumn(\"sentiment\", lit(1))\n",
        "tmp = negative_df.withColumn(\"sentiment\", lit(0))\n",
        "data = data.union(tmp)\n",
        "data.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------+---------+\n",
            "|         original_text|sentiment|\n",
            "+----------------------+---------+\n",
            "|  Now all @Apple ha...|        1|\n",
            "|  @Apple will be ad...|        1|\n",
            "|  Hilarious @youtub...|        1|\n",
            "|  @RIM you made it ...|        1|\n",
            "|  I just realized t...|        1|\n",
            "|  I'm a current @Bl...|        1|\n",
            "|  The 16 strangest ...|        1|\n",
            "|  Great up close & ...|        1|\n",
            "|  From which compan...|        1|\n",
            "|  Just apply for a ...|        1|\n",
            "|  RT @JamaicanIdler...|        1|\n",
            "|  Lmao I think @app...|        1|\n",
            "|  RT @PhillipRowntr...|        1|\n",
            "|  Wow. Great deals ...|        1|\n",
            "|  Just registered a...|        1|\n",
            "|你好 ! Currently le...|        1|\n",
            "|  Come to the dark ...|        1|\n",
            "|  Hey @apple, if yo...|        1|\n",
            "|  Thank you @apple ...|        1|\n",
            "|  Thanks to @Apple ...|        1|\n",
            "+----------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzrO7wi75EYI"
      },
      "source": [
        "### Stage01: Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N1gwE4a4B18"
      },
      "source": [
        "#### Step 01: Spelling Correction, Abbreviation & Emoji recovery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u1eZ0-f5o0G"
      },
      "source": [
        "#################################################################\n",
        "# STEP 01:                                                      #\n",
        "# Spelling correction, abbreviation recovery and emoji recovery.#\n",
        "#################################################################\n",
        "def spell_fix(s):\n",
        "\n",
        "  # Load and construct the abbreviation dictionary.\n",
        "  abbrDict = open(path02 + \"abbr.txt\").readlines()\n",
        "  src = []\n",
        "  dst = []\n",
        "  for line in abbrDict:\n",
        "  \tif len(line) > 1:\n",
        "  \t\tif line[0] != '#':\n",
        "  \t\t\tl = line.replace(\"\\n\", \"\").split(\"??\", 1)\n",
        "  \t\t\tsrc.append(l[0])\n",
        "  \t\t\tdst.append(l[1])\n",
        "     \n",
        "  # Replace appreviations.\n",
        "  for i in range(0, len(src)):\n",
        "    s = s.replace(src[i], dst[i])\n",
        "\n",
        "  # Suggest the words.\n",
        "  checker = SpellChecker(\"en_US\", filters=[EmailFilter, URLFilter])\n",
        "  checker.set_text(s.lower())\n",
        "  for err in checker:\n",
        "    suggestions = err.suggest()\n",
        "    if len(suggestions) != 0:\n",
        "\n",
        "      # Consider weighting the suggestions with respect to frequencies!\n",
        "      err.replace(err.suggest()[0])\n",
        "  return checker.get_text().lower()\n",
        "\n",
        "spark.udf.register(\"spell_fix_udf\", spell_fix)\n",
        "spell_fix_sql = SQLTransformer(statement = \"SELECT *, spell_fix_udf(original_text) as text FROM __THIS__\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vqdROEX4KSs"
      },
      "source": [
        "#### Step02: Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxdlmXihxvIF"
      },
      "source": [
        "#################################################################\n",
        "# STEP 02:                                                      #\n",
        "# Gain Tokens                                                   #\n",
        "#################################################################\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "sentence_detector = SentenceDetector() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\") \\\n",
        "    .setUseAbbreviations(True)\n",
        "    \n",
        "tokenizer = Tokenizer() \\\n",
        "  .setInputCols([\"sentence\"]) \\\n",
        "  .setOutputCol(\"token\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkWc9q3T4R55"
      },
      "source": [
        "#### Step 03: Stopwords & Noise Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oak94ERD0c29"
      },
      "source": [
        "#################################################################\n",
        "# STEP 03:                                                      #\n",
        "# Remove stopword and noise                                     #\n",
        "#################################################################\n",
        "# Encapsulate the class for noise removing\n",
        "# Remove @, link, #, RT, :\n",
        "class RegexTransformer(Transformer):\n",
        "  def __init__(self):\n",
        "    super(RegexTransformer, self).__init__()\n",
        "  def _transform(self, df):\n",
        "    df = df.withColumn('text_explode', F.explode(F.col('clean_token')))\\\n",
        "           .na.replace('', None)\\\n",
        "           .na.drop()\\\n",
        "           .withColumn('reg_ex', F.regexp_replace(F.col('text_explode'), '@\\w+', ''))\\\n",
        "           .withColumn('reg_ex', F.regexp_replace(F.col('reg_ex'),  r'http\\S+', ''))\\\n",
        "           .withColumn('reg_ex', F.regexp_replace(F.col('reg_ex'),  '#', ''))\\\n",
        "           .withColumn('reg_ex', F.regexp_replace(F.col('reg_ex'),  'RT', ''))\\\n",
        "           .withColumn('reg_ex', F.regexp_replace(F.col('reg_ex'),  ':', ''))\\\n",
        "           .groupBy('text').agg(F.collect_list(F.col('reg_ex')).alias('regex_array'))\n",
        "    return df\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "    .setInputCols(\"token\")\\\n",
        "    .setOutputCol(\"stopTokens\")\\\n",
        "    .setCaseSensitive(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY31725J4VsW"
      },
      "source": [
        "#### Step 04: Word Lemmatisation & Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yc2Udnv7YsG"
      },
      "source": [
        "#################################################################\n",
        "# STEP 04:                                                     #\n",
        "# Word Lemmatization & normalize                                #\n",
        "#################################################################\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"stopTokens\"]) \\\n",
        "    .setOutputCol(\"stem\")\n",
        "    \n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"stem\"]) \\\n",
        "    .setOutputCol(\"normalized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCYDw4Qh4ayc"
      },
      "source": [
        "#### Pipeline Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bEcEWB25bCa"
      },
      "source": [
        "#################################################################\n",
        "# Text mining pipeline construction.                            #\n",
        "#################################################################\n",
        "\n",
        "# Data transformers\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCols([\"clean_token\"]) \\\n",
        "    .setOutputAsArray(True) \\\n",
        "    .setCleanAnnotations(True)\n",
        "\n",
        "# Our NLP pipeline\n",
        "nlp_pipeline = Pipeline(stages=[spell_fix_sql, \n",
        "                                document_assembler, \n",
        "                                sentence_detector, \n",
        "                                tokenizer, \n",
        "                                stopwords_cleaner, \n",
        "                                stemmer, \n",
        "                                normalizer, \n",
        "                                finisher])\n",
        "\n",
        "# Fit and transform data\n",
        "def textMining(data):\n",
        "  model = nlp_pipeline.fit(data)\n",
        "  transformed_data = model.transform(data)\n",
        "\n",
        "  transformed_data.printSchema()\n",
        "  transformed_data.show()\n",
        "  return transformed_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN6WnY9mDZ86"
      },
      "source": [
        "### Stage02: Feature Extraction\n",
        "- TF-IDF\n",
        "- Word2Vec\n",
        "- Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er8mCsed14Fv"
      },
      "source": [
        "#### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00RYAjXl174L"
      },
      "source": [
        "def tfIdf(transformed_data):\n",
        "  # TF\n",
        "  hashingTF = HashingTF(inputCol=\"clean_token\", outputCol=\"rawFeatures\", numFeatures=200)\n",
        "  featurizedData = hashingTF.transform(transformed_data).persist()\n",
        "\n",
        "  #IDF\n",
        "  idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "  idfModel = idf.fit(featurizedData)\n",
        "  tmpData = idfModel.transform(featurizedData).persist()\n",
        "  \n",
        "  # Scale features\n",
        "  featureScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(tmpData)\n",
        "  processed = featureScaler.transform(tmpData)\n",
        "\n",
        "  # Trans & select data\n",
        "  df = processed.select(col(\"sentiment\").alias(\"labels\"), col(\"clean_token\").alias(\"MeaningfulWords\"), col(\"scaledFeatures\").alias(\"features\"))\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glu21yJ12oNE"
      },
      "source": [
        "#### word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J48Xo7LVQRSV"
      },
      "source": [
        "def myWord2Vec(transformed_data):\n",
        "  # Word2vec\n",
        "  word2vec = Word2Vec(vectorSize=200, seed=42, inputCol=\"clean_token\", outputCol=\"features\")\n",
        "  model = word2vec.fit(transformed_data)\n",
        "  tmpData = model.transform(transformed_data)\n",
        "\n",
        "  # Scale features\n",
        "  featureScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(tmpData)\n",
        "  processed = featureScaler.transform(tmpData)\n",
        "\n",
        "  # Trans & select data\n",
        "  df = processed.select(col(\"sentiment\").alias(\"labels\"), col(\"clean_token\").alias(\"MeaningfulWords\"), col(\"scaledFeatures\").alias(\"features\"))\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew3mu63DAYgG"
      },
      "source": [
        "#### Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfqG6pdmAcw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b1c9ce-9421-4499-cca2-71c3035690f3"
      },
      "source": [
        "# Load Bert model\n",
        "bert_embeddings = BertEmbeddings\\\n",
        "      .pretrained('bert_base_cased', 'en') \\\n",
        "      .setInputCols([\"document\",'normalized'])\\\n",
        "      .setOutputCol(\"bert\")\\\n",
        "      .setCaseSensitive(False)\\\n",
        "\n",
        "# Trans embedding words to embedding sentence\n",
        "embeddingsSentence = SentenceEmbeddings() \\\n",
        "      .setInputCols([\"document\", \"bert\"]) \\\n",
        "      .setOutputCol(\"sentence_embeddings\") \\\n",
        "      .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "# Finish    \n",
        "embeddings_finisher = EmbeddingsFinisher() \\\n",
        "      .setInputCols([\"sentence_embeddings\"]) \\\n",
        "      .setOutputCols([\"finished_sentence_embeddings\"]) \\\n",
        "      .setOutputAsVector(True)\\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "\n",
        "def myBert(data):\n",
        "  # Pipeline for bert\n",
        "  nlp_pipeline_bert = Pipeline(\n",
        "      stages=[spell_fix_sql,\n",
        "              document_assembler,\n",
        "              sentence_detector, \n",
        "              tokenizer, \n",
        "              stopwords_cleaner,\n",
        "              stemmer, \n",
        "              normalizer, \n",
        "              bert_embeddings,\n",
        "              embeddingsSentence,\n",
        "              embeddings_finisher])\n",
        "\n",
        "  # Feed data\n",
        "  nlp_model_bert = nlp_pipeline_bert.fit(data)\n",
        "  processed_bert = nlp_model_bert.transform(data)\n",
        "  processed_bert= processed_bert.withColumn(\"features\", explode(processed_bert.finished_sentence_embeddings))\n",
        "\n",
        "  # Scale features\n",
        "  featureScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(processed_bert)\n",
        "  processed = featureScaler.transform(processed_bert)\n",
        "\n",
        "  # Trans & select data\n",
        "  df = processed.select(col(\"sentiment\").alias(\"labels\"), col(\"normalized\").alias(\"MeaningfulWords\"), col(\"scaledFeatures\").alias(\"features\"))\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXS8mfONS97y"
      },
      "source": [
        "### Stage03: Model Training\n",
        "- Logistic Regression\n",
        "- Linear SVM\n",
        "- Naive Bayes\n",
        "- MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XXVfD2Usujl"
      },
      "source": [
        "### Divide data\n",
        "- 70% for training, 30% for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW85rK5jMxVj"
      },
      "source": [
        "def myDivideData(df):\n",
        "  dividedData = df.randomSplit([0.7, 0.3], 123) \n",
        "  trainingData = dividedData[0] #index 0 = data training\n",
        "  testingData = dividedData[1] #index 1 = data testing\n",
        "\n",
        "  return trainingData, testingData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb8_iA2FNbZG"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqQl1vx7NEJq"
      },
      "source": [
        "def myLogReg(trainingData, testingData):\n",
        "  # Training\n",
        "  lr = LogisticRegression(labelCol=\"labels\", featuresCol=\"features\", \n",
        "                        maxIter=10, regParam=0.01)\n",
        "  model = lr.fit(trainingData)\n",
        "\n",
        "  # Testing\n",
        "  prediction = model.transform(testingData)\n",
        "  predictionFinal = prediction.select(\n",
        "      \"MeaningfulWords\", \"prediction\", \"labels\")\n",
        "\n",
        "  # Analyse\n",
        "  correctPrediction = predictionFinal.filter(\n",
        "      predictionFinal['prediction'] == predictionFinal['labels']).count()\n",
        "  totalData = predictionFinal.count()\n",
        "  print(\"Logistic Regression Accuracy:\", correctPrediction/totalData)\n",
        "  \n",
        "  return correctPrediction/totalData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vWqlZvONkX1"
      },
      "source": [
        "#### Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3mz94tHNpA4"
      },
      "source": [
        "def myLSVM(trainingData, testingData):\n",
        "  # Training\n",
        "  lsvm = LinearSVC(labelCol=\"labels\", featuresCol=\"features\", \n",
        "                          maxIter=10, regParam=0.01)\n",
        "  model = lsvm.fit(trainingData)\n",
        "\n",
        "  # Testing\n",
        "  prediction = model.transform(testingData)\n",
        "  predictionFinal = prediction.select(\n",
        "      \"MeaningfulWords\", \"prediction\", \"labels\")\n",
        "\n",
        "  # Analyse\n",
        "  correctPrediction = predictionFinal.filter(\n",
        "      predictionFinal['prediction'] == predictionFinal['labels']).count()\n",
        "  totalData = predictionFinal.count()\n",
        "  print(\"Linear SVM Accuracy:\", correctPrediction/totalData)\n",
        "\n",
        "  return correctPrediction/totalData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2ka8gxtNtSm"
      },
      "source": [
        "#### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73gQ5Rs4NxPM"
      },
      "source": [
        "def myNBayes(trainingData, testingData):\n",
        "  # Training\n",
        "  nb = NaiveBayes(labelCol=\"labels\", featuresCol=\"features\",\n",
        "                  smoothing=1.0, modelType=\"multinomial\")\n",
        "  model = nb.fit(trainingData)\n",
        "\n",
        "  # Testing\n",
        "  prediction = model.transform(testingData)\n",
        "  predictionFinal = prediction.select(\n",
        "      \"MeaningfulWords\", \"prediction\", \"labels\")\n",
        "\n",
        "  # Analyse\n",
        "  correctPrediction = predictionFinal.filter(\n",
        "      predictionFinal['prediction'] == predictionFinal['labels']).count()\n",
        "  totalData = predictionFinal.count()\n",
        "  print(\"Naive Bayes Accuracy:\", correctPrediction/totalData)\n",
        "  \n",
        "  return correctPrediction/totalData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTWWxwiN0bH"
      },
      "source": [
        "#### MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAMXUQtAN2cf"
      },
      "source": [
        "def myMLP(trainingData, testingData, imputNumber):\n",
        "  # Define layer structure\n",
        "  layers = [imputNumber, 100, 100, 2]\n",
        "\n",
        "  # Training\n",
        "  mlp = MultilayerPerceptronClassifier(labelCol=\"labels\", featuresCol=\"features\", \n",
        "                                       maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
        "  model = mlp.fit(trainingData)\n",
        "\n",
        "  # Testing\n",
        "  prediction = model.transform(testingData)\n",
        "  predictionFinal = prediction.select(\n",
        "      \"MeaningfulWords\", \"prediction\", \"labels\")\n",
        "\n",
        "  # Analyse\n",
        "  correctPrediction = predictionFinal.filter(\n",
        "      predictionFinal['prediction'] == predictionFinal['labels']).count()\n",
        "  totalData = predictionFinal.count()\n",
        "  print(\"MLP Accuracy:\", correctPrediction/totalData)\n",
        "  \n",
        "  return correctPrediction/totalData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaetiFPT7jMe"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YijJpEaHIvgZ"
      },
      "source": [
        "#### Experiments Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRcrl1yNIfrd"
      },
      "source": [
        "**General experiments setup functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H5m11WJFKoH"
      },
      "source": [
        "def mlAlgorithms(df, isBert):\n",
        "  # Divide data\n",
        "  trainingData, testingData = myDivideData(df)\n",
        "\n",
        "  # Logistic Regression\n",
        "  myLogReg(trainingData, testingData)\n",
        "\n",
        "  # Linear SVM\n",
        "  myLSVM(trainingData, testingData)\n",
        "\n",
        "  # Naive Bayes\n",
        "  myNBayes(trainingData, testingData)\n",
        "\n",
        "  # MLP\n",
        "  if not isBert:\n",
        "    myMLP(trainingData, testingData, 200)\n",
        "  else:\n",
        "    myMLP(trainingData, testingData, 768)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpOFTkpTEuc_"
      },
      "source": [
        "def experiments(isControl, data):\n",
        "  # Pre-processing\n",
        "  if isControl:\n",
        "    transformed_data = textMiningControl(data)\n",
        "  else:\n",
        "    transformed_data = textMining(data)\n",
        "\n",
        "  # TF-IDF\n",
        "  print(\"#######################################################\")\n",
        "  print(\"TF-IDF:\")\n",
        "  df = tfIdf(transformed_data)\n",
        "  mlAlgorithms(df, False)\n",
        "\n",
        "  # word2vec\n",
        "  print(\"\\n#######################################################\")\n",
        "  print(\"word2vec:\")\n",
        "  df = myWord2Vec(transformed_data)\n",
        "  mlAlgorithms(df, False)\n",
        "\n",
        "  # Bert\n",
        "  print(\"\\n#######################################################\")\n",
        "  print(\"Bert:\")\n",
        "  if isControl:\n",
        "    df = myBert_control(data)\n",
        "  else:\n",
        "    df = myBert(data)\n",
        "  mlAlgorithms(df, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wny82GwZC4bO"
      },
      "source": [
        "**Baseline framework helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-u3pp5w7ns7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d8dcf8-2d7a-4edc-bb57-6f1ca9d1c012"
      },
      "source": [
        "################################################################################\n",
        "# Setup Control pipeline                                                       #\n",
        "################################################################################\n",
        "\n",
        "# Data transformers\n",
        "document_assembler_control = DocumentAssembler() \\\n",
        "    .setInputCol(\"original_text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "sentence_detector_control = SentenceDetector() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\") \\\n",
        "    .setUseAbbreviations(True)\n",
        "    \n",
        "tokenizer_control = Tokenizer() \\\n",
        "  .setInputCols([\"sentence\"]) \\\n",
        "  .setOutputCol(\"token\")\n",
        "\n",
        "finisher_control = Finisher() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCols([\"clean_token\"]) \\\n",
        "    .setOutputAsArray(True) \\\n",
        "    .setCleanAnnotations(True)\n",
        "\n",
        "# Pipeline for control group\n",
        "control_pipeline = Pipeline(stages=[document_assembler_control,\n",
        "                                    sentence_detector_control,\n",
        "                                    tokenizer_control,\n",
        "                                    finisher_control])\n",
        "# Fit and transform data\n",
        "def textMiningControl(data):\n",
        "  model = control_pipeline.fit(data)\n",
        "  transformed_data = model.transform(data)\n",
        "\n",
        "  transformed_data.printSchema()\n",
        "  transformed_data.show()\n",
        "  return transformed_data\n",
        "\n",
        "################################################################################\n",
        "# Bert for control group                                                       #\n",
        "################################################################################\n",
        "\n",
        "# Load Bert model\n",
        "bert_embeddings = BertEmbeddings\\\n",
        "      .pretrained('bert_base_cased', 'en') \\\n",
        "      .setInputCols([\"document\",'token'])\\\n",
        "      .setOutputCol(\"bert\")\\\n",
        "      .setCaseSensitive(False)\\\n",
        "\n",
        "# Trans embedding words to embedding sentence\n",
        "embeddingsSentence = SentenceEmbeddings() \\\n",
        "      .setInputCols([\"document\", \"bert\"]) \\\n",
        "      .setOutputCol(\"sentence_embeddings\") \\\n",
        "      .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "# Finish    \n",
        "embeddings_finisher = EmbeddingsFinisher() \\\n",
        "      .setInputCols([\"sentence_embeddings\"]) \\\n",
        "      .setOutputCols([\"finished_sentence_embeddings\"]) \\\n",
        "      .setOutputAsVector(True)\\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "\n",
        "def myBert_control(data):\n",
        "  # Pipeline for bert\n",
        "  nlp_pipeline_bert = Pipeline(\n",
        "      stages=[document_assembler_control,\n",
        "              sentence_detector_control,\n",
        "              tokenizer_control, \n",
        "              bert_embeddings,\n",
        "              embeddingsSentence,\n",
        "              embeddings_finisher])\n",
        "\n",
        "  # Feed data\n",
        "  nlp_model_bert = nlp_pipeline_bert.fit(data)\n",
        "  processed_bert = nlp_model_bert.transform(data)\n",
        "  processed_bert= processed_bert.withColumn(\"features\", explode(processed_bert.finished_sentence_embeddings))\n",
        "\n",
        "  # Scale features\n",
        "  featureScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(processed_bert)\n",
        "  processed = featureScaler.transform(processed_bert)\n",
        "\n",
        "  # Trans & select data\n",
        "  df = processed.select(col(\"sentiment\").alias(\"labels\"), col(\"token\").alias(\"MeaningfulWords\"), col(\"scaledFeatures\").alias(\"features\"))\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMsV23yWI_TM"
      },
      "source": [
        "#### Proposed Framework Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d65amtW-Epw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd89377-5f7f-432a-b791-986bf60acc65"
      },
      "source": [
        "isControl = False\n",
        "experiments(isControl, data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- original_text: string (nullable = true)\n",
            " |-- sentiment: integer (nullable = false)\n",
            " |-- text: string (nullable = true)\n",
            " |-- clean_token: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+----------------------+---------+----------------------+----------------------+\n",
            "|         original_text|sentiment|                  text|           clean_token|\n",
            "+----------------------+---------+----------------------+----------------------+\n",
            "|  Now all @Apple ha...|        1|  now all @laypeopl...|  [laypeopl, get, s...|\n",
            "|  @Apple will be ad...|        1|  @laypeople will b...|  [laypeopl, ad, ca...|\n",
            "|  Hilarious @youtub...|        1|  hilarious @you tu...|  [hilari, you, tub...|\n",
            "|  @RIM you made it ...|        1|  @rim you made it ...|  [rim, made, easi,...|\n",
            "|  I just realized t...|        1|  i just realized t...|  [realiz, reason, ...|\n",
            "|  I'm a current @Bl...|        1|  i'm a current @bl...|  [current, blackbe...|\n",
            "|  The 16 strangest ...|        1|  the 16 strangest ...|  [strangest, thing...|\n",
            "|  Great up close & ...|        1|  great up close & ...|  [great, close, pe...|\n",
            "|  From which compan...|        1|  from which compan...|  [compani, cheeki,...|\n",
            "|  Just apply for a ...|        1|  just laypeople fo...|  [laypeopl, job, l...|\n",
            "|  RT @JamaicanIdler...|        1|  rt @jamaican: loa...|  [rt, jamaican, lo...|\n",
            "|  Lmao I think @app...|        1|  loam i think @lay...|  [loam, think, lay...|\n",
            "|  RT @PhillipRowntr...|        1|  rt @philanthropis...|  [rt, philanthropi...|\n",
            "|  Wow. Great deals ...|        1|  wow. great deals ...|  [wow, great, deal...|\n",
            "|  Just registered a...|        1|  just registered a...|  [regist, laypeopl...|\n",
            "|你好 ! Currently le...|        1|你好 ! currently le...|[你好, current, lea...|\n",
            "|  Come to the dark ...|        1|  come to the dark ...|  [come, dark, side...|\n",
            "|  Hey @apple, if yo...|        1|  hey @laypeople, i...|  [hei, laypeopl, s...|\n",
            "|  Thank you @apple ...|        1|  thank you @laypeo...|  [thank, laypeopl,...|\n",
            "|  Thanks to @Apple ...|        1|  thanks to @laypeo...|  [thank, laypeopl,...|\n",
            "+----------------------+---------+----------------------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "#######################################################\n",
            "TF-IDF:\n",
            "Logistic Regression Accuracy: 0.7872340425531915\n",
            "Linear SVM Accuracy: 0.8085106382978723\n",
            "Naive Bayes Accuracy: 0.7943262411347518\n",
            "MLP Accuracy: 0.8014184397163121\n",
            "\n",
            "#######################################################\n",
            "word2vec:\n",
            "Logistic Regression Accuracy: 0.7304964539007093\n",
            "Linear SVM Accuracy: 0.6950354609929078\n",
            "Naive Bayes Accuracy: 0.6453900709219859\n",
            "MLP Accuracy: 0.7021276595744681\n",
            "\n",
            "#######################################################\n",
            "Bert:\n",
            "Logistic Regression Accuracy: 0.8014184397163121\n",
            "Linear SVM Accuracy: 0.7801418439716312\n",
            "Naive Bayes Accuracy: 0.7801418439716312\n",
            "MLP Accuracy: 0.7588652482269503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-NzMrZqJpwq"
      },
      "source": [
        "#### Baseline Framework Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9QfNaaTJtzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8b6fd7-9dde-426c-fd94-28f85fc938ae"
      },
      "source": [
        "isControl = True\n",
        "experiments(isControl, data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- original_text: string (nullable = true)\n",
            " |-- sentiment: integer (nullable = false)\n",
            " |-- clean_token: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+----------------------+---------+----------------------+\n",
            "|         original_text|sentiment|           clean_token|\n",
            "+----------------------+---------+----------------------+\n",
            "|  Now all @Apple ha...|        1|  [Now, all, @Apple...|\n",
            "|  @Apple will be ad...|        1|  [@Apple, will, be...|\n",
            "|  Hilarious @youtub...|        1|  [Hilarious, @yout...|\n",
            "|  @RIM you made it ...|        1|  [@RIM, you, made,...|\n",
            "|  I just realized t...|        1|  [I, just, realize...|\n",
            "|  I'm a current @Bl...|        1|  [I'm, a, current,...|\n",
            "|  The 16 strangest ...|        1|  [The, 16, strange...|\n",
            "|  Great up close & ...|        1|  [Great, up, close...|\n",
            "|  From which compan...|        1|  [From, which, com...|\n",
            "|  Just apply for a ...|        1|  [Just, apply, for...|\n",
            "|  RT @JamaicanIdler...|        1|  [RT, @JamaicanIdl...|\n",
            "|  Lmao I think @app...|        1|  [Lmao, I, think, ...|\n",
            "|  RT @PhillipRowntr...|        1|  [RT, @PhillipRown...|\n",
            "|  Wow. Great deals ...|        1|  [Wow, ., Great, d...|\n",
            "|  Just registered a...|        1|  [Just, registered...|\n",
            "|你好 ! Currently le...|        1|[你好, !, Currently...|\n",
            "|  Come to the dark ...|        1|  [Come, to, the, d...|\n",
            "|  Hey @apple, if yo...|        1|  [Hey, @apple, ,, ...|\n",
            "|  Thank you @apple ...|        1|  [Thank, you, @app...|\n",
            "|  Thanks to @Apple ...|        1|  [Thanks, to, @App...|\n",
            "+----------------------+---------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "#######################################################\n",
            "TF-IDF:\n",
            "Logistic Regression Accuracy: 0.6808510638297872\n",
            "Linear SVM Accuracy: 0.6737588652482269\n",
            "Naive Bayes Accuracy: 0.6524822695035462\n",
            "MLP Accuracy: 0.6312056737588653\n",
            "\n",
            "#######################################################\n",
            "word2vec:\n",
            "Logistic Regression Accuracy: 0.6666666666666666\n",
            "Linear SVM Accuracy: 0.6099290780141844\n",
            "Naive Bayes Accuracy: 0.6028368794326241\n",
            "MLP Accuracy: 0.6595744680851063\n",
            "\n",
            "#######################################################\n",
            "Bert:\n",
            "Logistic Regression Accuracy: 0.8368794326241135\n",
            "Linear SVM Accuracy: 0.7304964539007093\n",
            "Naive Bayes Accuracy: 0.7375886524822695\n",
            "MLP Accuracy: 0.8085106382978723\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}